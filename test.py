import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# S·ª≠ d·ª•ng GPU n·∫øu c√≥
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ƒê∆∞·ªùng d·∫´n t·ªõi m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán
MODEL_PATH = "./trained_model2"

# Load tokenizer v√† m√¥ h√¨nh
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(device)

# Prompt h∆∞·ªõng d·∫´n chuy√™n bi·ªát
INSTRUCTION = (
    "You are a helpful, concise medical assistant. You specialize in diagnosing conditions "
    "based on user-described symptoms. Always try to guess possible conditions and give advice "
    "on what to do next. Avoid repeating or unrelated information. Answer clearly and professionally.\n"
)

# H√†m ƒë·ªÉ h·ªèi m√¥ h√¨nh
def chat_with_model(question, max_new_tokens=256, temperature=0.7, top_p=0.9):
    prompt = f"{INSTRUCTION}Patient: {question.strip()}\nDoctor:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        output = model.generate(
            inputs["input_ids"],
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_k=50,
            top_p=top_p,
            temperature=temperature,
            pad_token_id=tokenizer.eos_token_id,
            attention_mask=inputs.get("attention_mask")
        )

    response = tokenizer.decode(output[0], skip_special_tokens=True)

    # Ch·ªâ gi·ªØ ph·∫ßn tr·∫£ l·ªùi c·ªßa b√°c sƒ© sau "Doctor:"
    if "Doctor:" in response:
        answer = response.split("Doctor:")[-1].strip()
    else:
        answer = response.strip()

    # K·∫øt th√∫c b·∫±ng d·∫•u ch·∫•m n·∫øu thi·∫øu
    if not answer.endswith(('.', '!', '?')):
        answer += '.'

    return answer

# Giao di·ªán d√≤ng l·ªánh
if __name__ == "__main__":
    print("ü§ñ B√°c sƒ© AI s·∫µn s√†ng (g√µ 'exit' ƒë·ªÉ tho√°t)")
    while True:
        user_input = input("üë§ B·∫°n: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üëã T·∫°m bi·ªát!")
            break
        reply = chat_with_model(user_input)
        print("ü§ñ B√°c sƒ© AI:", reply)
