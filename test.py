import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Thiáº¿t bá»‹
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ÄÆ°á»ng dáº«n tá»›i model Ä‘Ã£ huáº¥n luyá»‡n
MODEL_PATH = "./trained_model2"  # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n

# Load tokenizer vÃ  model
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH).to(device)

# HÃ m Ä‘á»ƒ há»i mÃ´ hÃ¬nh
def chat_with_model(question, max_new_tokens=256, temperature=0.7, top_p=0.95):
    # Táº¡o prompt vá»›i máº«u cÃ¢u tá»± nhiÃªn
    prompt = f"Question: {question.strip()} Answer:"

    # Token hÃ³a, vá»›i attention_mask rÃµ rÃ ng
    inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True).to(device)

    # Sinh vÄƒn báº£n
    with torch.no_grad():
        outputs = model.generate(
            inputs["input_ids"],
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_k=50,
            top_p=top_p,
            temperature=temperature,
            pad_token_id=tokenizer.eos_token_id,
            attention_mask=inputs.get("attention_mask")  # Äáº£m báº£o attention mask Ä‘Æ°á»£c truyá»n
        )

    # Decode káº¿t quáº£
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # TÃ¡ch pháº§n tráº£ lá»i sau tá»« "Answer:"
    answer = response.split("Answer:")[-1].strip()

    # Äáº£m báº£o cÃ¢u tráº£ lá»i káº¿t thÃºc há»£p lÃ½ (thÃªm dáº¥u cÃ¢u náº¿u thiáº¿u)
    if not answer.endswith(('.', '!', '?')):
        answer += '.'

    return answer

# Giao diá»‡n CLI Ä‘Æ¡n giáº£n
if __name__ == "__main__":
    print("ğŸ¤– Chatbot Y Táº¿ (gÃµ 'exit' Ä‘á»ƒ thoÃ¡t)")
    while True:
        user_input = input("ğŸ‘¤ Báº¡n: ")
        if user_input.lower() in ["exit", "quit"]:
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break
        reply = chat_with_model(user_input)
        print("ğŸ¤– BÃ¡c sÄ© AI:", reply)
